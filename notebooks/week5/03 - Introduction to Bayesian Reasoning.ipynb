{
 "metadata": {
  "name": "",
  "signature": "sha256:9c3954d67c14487e9de146c9f8736b3b115da3a60344609bce8c7875f0a00610"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bayesian Inference\n",
      "\n",
      "Let's say you have three coins. Two are normal, and one is weighted so it always flips heads. You pick up a coin at random and flip it three times, getting heads each time. _What is the chance it's the weighted coin?_\n",
      "\n",
      "If you think it's higher than 1/3, then you are thinking Bayesian! Bayesian inference is simply updating your beliefs after considering new evidence. A Bayesian can rarely be certain about a result, but he or she can be very confident. Just like in the example above, we can never be 100% sure that we have the weighted coin. But if we flip it many times and get heads each time, we can feel confident that we have the weighted coin, but not certain.  Bayesian inference works identically: we update our beliefs about an outcome; rarely can we be absolutely sure unless we rule out all other alternatives. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The Bayesian state of mind\n",
      "\n",
      "Bayesian inference differs from more traditional statistical inference by preserving *uncertainty*. The Bayesian world-view interprets probability as measure of *believability in an event*, that is, how confident we are in an event occurring. \n",
      "\n",
      "An individual who assigns a belief of 0 to an event has no confidence that the event will occur; conversely, assigning a belief of 1 implies that the individual is absolutely certain of an event occurring. Beliefs between 0 and 1 allow for weightings of other outcomes.\n",
      "\n",
      "Notice in the paragraph above, I assigned the belief (probability) measure to an *individual*. This definition leaves room for conflicting beliefs between individuals. This is appropriate for what naturally occurs: different individuals have different beliefs of events occurring, because they possess different *information* about the world. The existence of different beliefs does not imply that anyone is wrong. Consider the following examples demonstrating the relationship between individual beliefs and probabilities:\n",
      "\n",
      "- I flip a coin, and we both guess the result. We would both agree, assuming the coin is fair, that the probability of Heads is 1/2. Assume, then, that I peek at the coin. Now I know for certain what the result is: I assign probability 1.0 to either Heads or Tails (whichever it is). Now what is *your* belief that the coin is Heads? My knowledge of the outcome has not changed the coin's results. Thus we assign different probabilities to the result. \n",
      "\n",
      "- Your code either has a bug in it or not, but we do not know for certain which is true, though we have a belief about the presence or absence of a bug.  \n",
      "\n",
      "- A medical patient is exhibiting symptoms $x$, $y$ and $z$. There are a number of diseases that could be causing all of them, but only a single disease is present. A doctor has beliefs about which disease, but a second doctor may have slightly different beliefs. \n",
      "\n",
      "This philosophy of treating beliefs as probability is natural to humans. We employ it constantly as we interact with the world and only see partial truths, but gather evidence to form beliefs. \n",
      "\n",
      "Our belief about event $A$ is denoted as $P(A)$. This is called the *prior probability*, as it's the probability before we consider new evidence.\n",
      "\n",
      "John Maynard Keynes, a great economist and thinker, said \"When the facts change, I change my mind.\" This quote reflects the way a Bayesian updates his or her beliefs after seeing evidence. Even &mdash; especially &mdash; if the evidence is counter to what was initially believed, the evidence cannot be ignored. We denote our updated belief as $P(A |X )$, interpreted as the probability of $A$ given the evidence $X$. We call the updated belief the *posterior probability* so as to contrast it with the prior probability. \n",
      "\n",
      "When we chose a coin at random in the problem at the beginning of this notebook, we knew it had a 1 in 3 probability of being the weighted coin -- $P(A) = \\frac{1}{3}$. After flipping it three times and getting heads each time, we have new evidence and can calculate $P(A |X )$.\n",
      "\n",
      "Let's start with something simpler: we flip the coin once and get heads. $A$ is the prior probability of us having the weighted coin, $X$ is our new evidence (and the probability of it happening.)\n",
      "\n",
      "Now, in order to calculate $P(A |X )$, we need Bayes' theorem:\n",
      "\n",
      "\\begin{align}\n",
      " P( A | X ) = & \\frac{ P(X | A) P(A) } {P(X) }\n",
      "\\end{align}\n",
      "\n",
      "In English:\n",
      "\n",
      "> The probability of A given the evidence X is equal to the probability of X given A multiplied by the probability of A, divided by the probability of X.\n",
      "\n",
      "We already know the probability of A -- $\\frac{1}{3}$.\n",
      "\n",
      "To get the probability of X, we need to do some calculations. The likelihood of flipping a normal coin and getting heads is $\\frac{1}{2}$. The likelihood of flipping the weighted coin and getting heads is $1$. To calculate the likelihood of flipping a random coin from these three coins and getting heads, we have to get the average of their probabilities: $P(X) = \\frac{\\frac{1}{2} + \\frac{1}{2} + 1}{3} = \\frac{2}{3}$.\n",
      "\n",
      "Lastly, the probability of X given A. This seems like an odd one, but written out, it's the probability of getting heads (X) given that we know we have the weighted coin (A). That's 1!\n",
      "\n",
      "\\begin{align}\n",
      " P( A | X ) = & \\frac{ 1 \\times \\frac{1}{3} } {\\frac{2}{3} } = \\frac{1}{2}.\n",
      "\\end{align}\n",
      "\n",
      "The probability that we have the weighted coin is 0.5.\n",
      "\n",
      "The above formula is not unique to Bayesian inference: it is a mathematical fact with uses outside Bayesian inference. Bayesian inference merely uses it to connect prior probabilities $P(A)$ with an updated posterior probabilities $P(A | X )$.\n",
      "\n",
      "_Now calculate $P(A |X )$ if we have two heads in a row, and then for three heads in a row._"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## What is the point of this?\n",
      "\n",
      "A lot of what we do with machine learning can be derived from Bayesian inference. A spam filter is the classic example. Let's say we know given our previous email that 50% of our email is spam. We get an email with the word \"loan\" in it. We know that 1.5% of our spam has the word \"loan\" in it. We also know that only 1% of emails we get have the word \"loan\" in them. So what's the chance this email is spam?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A - email is spam\n",
      "# X - email has \"loan\" in it\n",
      "spam_chance = 0.5 # P(A)\n",
      "loan_chance = 0.01 # P(X)\n",
      "loan_in_spam_chance = 0.015 # P(X | A)\n",
      "\n",
      "(loan_in_spam_chance * spam_chance) / loan_chance"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "0.75"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Will you have to do this math?\n",
      "\n",
      "We have `scikit-learn` to help us with that. The math is not that hard for single feature questions, but gets complicated for multiple features."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# References and Further Reading\n",
      "\n",
      "* [Probabilistic Programming & Bayesian Methods for Hackers](https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/) - Much of this notebook came from their introduction.\n",
      "* [An Intuitive Explanation of Bayes' Theorem](http://www.yudkowsky.net/rational/bayes)\n",
      "* [Think Bayes](http://www.greenteapress.com/thinkbayes/)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}